# -*- coding: utf-8 -*-
"""Medical Insurance Dataset | Regression

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wTX7nNpmX5fFzwKs2UxPTgA5U5ueH2ku

# *Project Title: Medical Insurance Dataset*

---

### Importing Important Libraries

---

**We first import the required Python libraries:**

* **pandas** → for data handling and preprocessing
* **numpy** → for numerical and mathematical operations
* **matplotlib / seaborn** → for data visualization and exploratory analysis
* **scipy** → for statistical analysis and hypothesis testing
* **sklearn.preprocessing** → for encoding categorical features
* **sklearn.model_selection** → for splitting the dataset into training and testing sets
* **sklearn.linear_model, tree, ensemble, svm** → for implementing different regression models like Linear Regression, Decision Tree, Random Forest, Gradient Boosting, and Support Vector Regressor
"""

#Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

#stop warnings
import warnings
warnings.filterwarnings('ignore')

#preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

#Modeling
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR

# Metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

"""###Loading Dataset"""

df=pd.read_csv("/content/insurance.csv")
df

"""###Data Exploration

---

* **df.head() / df.tail()** → to view the first and last few records of the dataset and understand its basic structure.
* **df.shape** → to check the total number of rows and columns.
* **df.columns** → to list all column names present in the dataset.
* **df.info()** → to get details about data types, non-null counts, and memory usage.
* **df.describe()** → to generate summary statistics (mean, median, std, min, max) for numerical columns.
* **df.select_dtypes(include='number')** → to identify all numeric columns.
* **df.select_dtypes(include=['object','category'])** → to identify all categorical columns.

👉 *These steps help in understanding the dataset’s structure, data types, and basic statistical properties before performing further analysis.*

---



"""

df.head()

df.tail()

df.describe()

df.columns

df.shape

df.info()

# This code prints the lists of all numeric and categorical columns in the dataframe.
print("Numeric columns list:", df.select_dtypes(include='number').columns.tolist())
print("Categorical columns list:", df.select_dtypes(include=['object','category']).columns.tolist())

"""---

### Value Consistency
---

**Checking Value Consistency in Categorical Columns:**

* Used **`value_counts()`** to display the frequency of unique values in each categorical column.
* Helped identify **duplicate, inconsistent, or misspelled category names** (e.g., “Male” vs “male”).
* Ensured that all categorical data is **uniform and standardized** before encoding.
* This step improves the **accuracy and reliability** of model training.

"""

col=['sex', 'smoker', 'region']
for i in col:
  print(df[i].value_counts())

"""# Summary od Data Exploration

---


###1. Dataset Overview

* Dataset shape: 1338 rows × 7 columns

* Columns: age, sex, bmi, children, smoker, region, charges

Column types:

* Numeric: age, bmi, children, charges (4 columns)

* Categorical: sex, smoker, region (3 columns)

###2. Ranges of Numeric Coilumns
🔹 Age

* Range: 18 → 64 years


🔹 BMI (Body Mass Index)

* Range: 15.96 → 53.13

🔹 Children

* Range: 0 → 5


🔹 Charges

* Range: 1,121.87 → 63,770.43

# Data Cleaning

### Handling Missing Values
"""

df.isnull().sum()

"""###Handling Duplicated Values"""

df.duplicated().sum()

df = df.drop_duplicates()

df.duplicated().sum()

"""---


### 🧹 **Data Cleaning Summary**

---



* Checked for missing values using `df.isnull().sum()`  
  → ✅ No missing values found in any column (all columns have 0 nulls)
* Checked for duplicate rows using `df.duplicated().sum()`  
  → ⚠️ Found 1 duplicate row      
  → removed using `df = df.drop_duplicates()`
* Dataset is now clean, consistent, and ready for further analysis

###Inconsistency
"""

int_col=df.select_dtypes(include=['int64']).columns.tolist()
df[int_col]

float_col=df.select_dtypes(include=['float64']).columns.tolist()
df[float_col]

cat_col=df.select_dtypes(include=['object']).columns.tolist()
df[cat_col]

"""---

### 🔍 **Data Consistency & Type Check Summary**

---



* Checked consistency of **integer, float, and categorical** columns → all values were valid and consistent.
* Data Types:
  * **Integers:** age, children
  * **Floats:** bmi, charges
  * **Objects (categorical):** sex, smoker, region
* No need to change or convert any data types

# EDA
"""

df['charges'].describe()

"""### Histogram"""

df.hist('charges',bins = 5, color = ["pink"])

print(f"Skewness: {df['charges'].skew()}")
print(f"Kurtosis: {df['charges'].kurt()}")

"""---

### 🎯 **EDA on Target Column: `charges`**

---



* Used `df['charges'].describe()` to view summary statistics

  * **Range:** 1,121.87 → 63,770.43
  * **Mean:** 13,279.12
  * **Std Dev:** 12,110.36
  * **Median (50%):** 9,386.16
* Plotted **Histogram** using

  ```python
  df.hist('charges', bins=5, color=['pink'])
  ```

  → Distribution was **right-skewed**
* Calculated skewness & kurtosis:

  * **Skewness:** 1.51 → positively skewed distribution
  * **Kurtosis:** 1.60 → light-tailed distribution
* ✅ Indicates that higher medical charges occur less frequently but with large variation

###Relationship Between Variables
"""

import seaborn as sns

sns.pairplot(df, hue="smoker", palette="Set1", diag_kind="kde")
plt.show()

"""### Scatterplot -> Numerical + Numerical"""

numeric_cols = ['age', 'bmi', 'children']

for col in numeric_cols:
    plt.figure(figsize=(6,4))
    sns.scatterplot(x=col, y='charges', data=df, palette="Set2", alpha=0.7)
    plt.title(f"Scatter plot: {col} vs charges")
    plt.show()

"""### Countplot -> Categorical Only"""

categorical_cols = ['sex', 'smoker', 'region']

for col in categorical_cols:
    plt.figure(figsize=(6,4))
    sns.countplot(x=col, data=df, palette="Set2")
    plt.title(f'Countplot of {col}')
    plt.show()

"""###Catplot -> Categorical + Numeric (Target)"""

categorical_cols = ['sex', 'smoker', 'region']

for col in categorical_cols:
    sns.catplot(x=col, y='charges', data=df, kind='box', palette='Set2', height=5, aspect=1.5).fig.suptitle(f"Catplot: {col} vs charges", y=1.02)

"""---

### 📈 **Exploratory Data Analysis (EDA) — Visual Insights**

---



* Created multiple plots to explore relationships and distributions:

  * **Pairplot:**

    * Used to check relationships between all variables
  * **Scatterplots:**

    * Plotted numeric features (age, bmi, children) vs target (charges)
    * Helped visualize positive correlation for age & bmi with charges
  * **Catplots (Boxplots):**

    * Compared categorical features (sex, smoker, region) with charges
  * **Countplots:**

    * Displayed category distribution for sex, smoker, and region
  * ✅ All plots were generated in **loops** for efficiency and better comparison across multiple columns

### Distribution of Target Variable
"""

from scipy import stats

plt.subplots(figsize=(10,9))
sns.distplot(df['charges'], fit=stats.norm)

(mu, sigma) = stats.norm.fit(df['charges'])

# plot with the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')
plt.ylabel('Frequency')

df['charges']

"""###Applying Log Function for Normal Distribution"""

#we use log function which is in numpy
df['charges'] = np.log1p(df['charges'])

#Check again for more normal distribution
plt.subplots(figsize=(12,9))
sns.distplot(df['charges'], fit=stats.norm)

# Get the fitted parameters used by the function
(mu, sigma) = stats.norm.fit(df['charges'])

# plot with the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')
plt.ylabel('Frequency')

df['charges']

"""---

### 📊 **Distribution Analysis & Normalization**

---



* Plotted **distribution plot** of target variable `charges` using `sns.distplot()`
* Fitted a **normal distribution curve** using `stats.norm.fit()`

  * Observed **high mean (μ)** and **standard deviation (σ)** → indicating **non-normal, right-skewed data**
* Applied **log transformation** to normalize the distribution:

  ```python
  df['charges'] = np.log1p(df['charges'])
  ```
* Replotted the distribution after transformation → curve became more **symmetrical and closer to normal**
* ✅ Log transformation successfully reduced skewness and balanced the spread of values

---


### Dealing with Categorical Features

---

* Applied Label Encoding on categorical columns → sex, smoker, region
"""

# Extracting categorical columns:
catFeatures= [col for col in df.columns if col in
              df.select_dtypes(include=object).columns]

from sklearn.preprocessing import LabelEncoder

# Encoding Categorical Data
labelEncode = LabelEncoder()

# Iterating Over each categorial features:
for col in catFeatures:
    # storing its numerical value:
    df[col] = labelEncode.fit_transform(df[col])

"""###Correlation Plot"""

#Coralation plot
corr = df.corr()
plt.subplots(figsize=(10,5))
sns.heatmap(corr, annot=True)

print("Find most important features related to target")
corr = df.corr()
corr = corr.corr()
corr.sort_values(['charges'], ascending=False, inplace=True)
corr.charges

"""---

### 🔗 **Correlation Analysis (After Label Encoding)**

---



* Applied **Label Encoding** on categorical columns: `sex`, `smoker`, `region`
* Created **correlation heatmap** to visualize relationships between all numeric and encoded categorical features
* Sorted correlations with target column (`charges`) to find most and least impactful features

**📊 Correlation Results:**

* **smoker:** +0.74 → strong positive correlation
* **age:** +0.53 → moderate positive correlation
* **children:** –0.11 → weak negative correlation
* **bmi:** –0.23 → slight negative correlation
* **sex:** –0.38 → weak negative correlation
* **region:** –0.53 → moderate negative correlation

---

**🔝 Top Features Positively Affecting Charges:**

* smoker (+0.74)
* age (+0.53)

**🔻 Features Negatively Affecting Charges:**

* region (–0.53)
* sex (–0.38)
* bmi (–0.23)
* children (–0.11)

---

✅ **Insight:**

* `smoker` and `age` are the most influential features increasing medical charges.
* `region`, `sex`, `bmi`, and `children` have weaker or negative effects on charges.

---


### Feature and Target Variable Separation


---

* X (features): the target column (charges) was dropped, keeping only 6 features.
* y (target): charges
"""

y = df['charges']
#Take their values in X and y
X = df.drop('charges', axis = 1).values
y = y.values

"""

---


###Train-Test Split

---

###🔹 Data Split

* Train-test split: 80/20
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

X_test

X_train.shape

"""###Model Training"""

# ===================== DEFINE MODELS =====================
# A dictionary named models is created to store all five regression models.

models = {
    "LinearRegression": LinearRegression(),
    "DecisionTreeRegressor": DecisionTreeRegressor(),
    "RandomForestRegressor": RandomForestRegressor(n_estimators=100, random_state=42),
    "GradientBoostingRegressor": GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42),
    "SVR": SVR(kernel='rbf')
}

# ===================== TRAIN MODELS =====================
#This loop trains each model one by one using the training dataset (X_train, y_train).
print("------ TRAINING MODELS ------\n")
for name, model in models.items():
    model.fit(X_train, y_train)
    print(f"{name} trained.")

"""### Model Prediction and Accuracy Evaluation"""

# ----- Loop 2: Predict and check accuracy -----
# creates an empty dictionary because we need a place to store the accuracy values (acc) of each model while the loop runs.
results={}
for name, model in models.items():
    y_pred = model.predict([X_test[150]])
    y_real = y_test[150]
    acc = model.score(X_test, y_test) * 100

    results[name] = acc  # store accuracy for comparison graph

    print(f"Model: {name}")
    print(f"Predict Value: {y_pred}")
    print(f"Real Value: {y_real}")
    print(f"ACCURACY --> {acc}\n")

"""### Model Performance Metrics Calculation"""

# ===================== METRICS CALCULATION =====================
print("\n==================== REGRESSION METRICS ====================\n")

# Lopp for Metrics
for name, model in models.items():
    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    acc = model.score(X_test, y_test) * 100


#Displays all metrics for each model in a clean format on the console.
    print(f"\n------ {name} ------")
    print(f"Mean Absolute Error : {mae}")
    print(f"Mean Squared Error : {mse}")
    print(f"Root Mean Squared Error : {rmse}")
    print(f"R-squared : {r2}")
    print(f"ACCURACY --> {acc}\n")

"""#### Applying Anti-Log to Convert 'charges' Back to Original Scale"""

# Now apply anti-log to get back original values
df['charges'] = np.expm1(df['charges'])

# Check again the distribution after applying anti-log
plt.subplots(figsize=(12,9))
sns.distplot(df['charges'], fit=stats.norm)

# Get the fitted parameters used by the function
(mu, sigma) = stats.norm.fit(df['charges'])

# Plot with the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')
plt.title("Distribution of Price After Applying Anti-Log Transformation")
plt.ylabel('Frequency')
plt.xlabel('charges')
plt.show()

df['charges']

"""###Model Accuracy Comparison (Bar Chart Visualization)"""

# ===================== CONVERT RESULTS TO DATAFRAME =====================
#This converts the dictionary into a DataFrame — so that the results are displayed in a tabular form.
results_df = pd.DataFrame(list(results.items()), columns=["Model", "Accuracy"])
print("------ MODEL ACCURACY COMPARISON ------\n")
print(results_df)

# ===================== BAR CHART COMPARISON =====================
#Draws a horizontal bar chart using soft pastel colors for better readability.
plt.figure(figsize=(8,6))
# Soft pastel color palette
colors = ['#A1C9F4', '#FFB3AB', '#B4DEB0', '#FFD8B1', '#D8B7DD', '#B3E2CD']
bars=plt.barh(results_df["Model"], results_df["Accuracy"], color=colors)
plt.xlabel("Accuracy (R² Score %)")
plt.title("Model Accuracy Comparison (Bar Chart)")
plt.grid(axis='x', linestyle='--', alpha=0.6)

# Adds the accuracy percentage label next to each bar for clarity.
for bar in bars:
    width = bar.get_width()
    plt.text(width + 0.9, bar.get_y() + bar.get_height()/2,
             f"{width:.2f}%", va='center', fontsize=10)

plt.tight_layout()
plt.show()

"""###Regression Models Accuracy Comparison (Pie Chart Visualization)"""

#========= PIE CHART ==========
#Creates a new figure (plot window) with a size of 6×6 inches
plt.figure(figsize=(6,6))
#Defines the size of each slice based on model accuracy.
plt.pie(results_df["Accuracy"],
        #Displays each model’s name
        labels=results_df["Model"],
        #Shows the percentage value (e.g., “84.7%”) on each slice.
        autopct='%1.1f%%',
        #Uses light pastel colors
        colors=plt.cm.Pastel1.colors,
        #Rotates the chart to start at 90°
        startangle=90)
plt.title("Regression Models Accuracy Comparison (Pie Chart)")
plt.show()

"""# 🏆 Best Model: Gradient Boosting Regressor

* It achieved the highest accuracy (R² = 74.43%) and lowest errors among all models.

* Works best because it learns sequentially, reduces bias & variance, and handles complex (non-linear) data better.

###📈 Visual Results

* Bar Chart: Gradient Boosting bar was the tallest → best accuracy.

* Pie Chart: Largest slice for Gradient Boosting → top performer.

###🎯 Conclusion

Gradient Boosting Regressor is the best model overall, with the highest accuracy, lowest MAE/MSE/RMSE, and most consistent predictions compared to real values.

---


### 📘 **Summary of Medical Insurance Dataset**

---



#### 🧹 **Data Cleaning**

* Checked for **missing values** → none found.
* Verified **data consistency** in `int`, `float`, and `categorical` columns → no issues detected.

#### 🔠 **Data Type & Inconsistency Check**

* Used `select_dtypes()` to separate numeric and categorical columns.
* All data types were already correct → no conversion needed.

#### 📊 **Exploratory Data Analysis (EDA)**

* Created **pairplots, scatterplots, catplots, and countplots** to explore relationships and distributions.
* Used **loops** to visualize all categorical and numeric variables efficiently.

#### 📈 **Distribution & Transformation**

* Plotted the distribution of `charges` using `sns.distplot()`.
* Found **high μ (mean)** and **σ (std)** values → applied **log transformation (`np.log1p`)** to normalize the data.
* Distribution became smoother and more normal after transformation.

#### 🔗 **Correlation Analysis**

* Generated **correlation heatmap** to check relationships between variables.
* Found key features affecting the target (`charges`):

  * **Top positive correlation:** `smoker`, `age`
  * **Least correlation:** `region`, `sex`
* Label Encoding applied before correlation for categorical variables.

#### 🧩 **Model Building**

* Split data using `train_test_split()` (80% train, 20% test).
* Trained 5 regression models:

  * Linear Regression
  * Decision Tree Regressor
  * Random Forest Regressor
  * Gradient Boosting Regressor
  * SVR

#### 🔍 **Model Evaluation**

* Compared **predicted vs actual values** for each model.
* Calculated metrics: **MAE, MSE, RMSE, R² (Accuracy)**.
* Created **bar chart & pie chart** for model accuracy comparison.

#### 🥇 **Best Model**

* **Gradient Boosting Regressor** performed best with:

  * **Accuracy:** 74.43%
  * **Lowest MAE (0.22) & RMSE (0.46)**
* Reason: Handles complex data, reduces bias & variance, and provides balanced learning.

#### 🔁 **Final Step**

* Applied **anti-log transformation (`np.expm1`)** to bring `charges` back to original scale.
* Rechecked distribution — appeared balanced and normally spread.

---

✅ **Final Conclusion:**
The project successfully cleaned, visualized, transformed, modeled, and evaluated data — concluding that **Gradient Boosting Regressor** is the most reliable and accurate model for predicting medical insurance charges.
"""

